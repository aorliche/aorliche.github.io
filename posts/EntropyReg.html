<h2>
    <span class='date'>Sun Oct 09 2022</span>
    <a class='title' href='/?post=EntropyReg'>Entropy Regularization: An Alternative to L1-Induced Sparsity</a>
    <a class='pin' href='#'>Hide</a>
</h2>
<div class='body'>
    <p>Most penalty terms that induce sparsity are based on the $p$ norm, where $0\leq p\leq1$. An example is the L1 norm shown below.</p>
    <div class='figure'>
        <div id='l1-graph' style='display: inline-block;'></div>
        <div id='l2-graph' style='display: inline-block;'></div>
        <div id='entropy-contour' style='display: inline-block;'></div>
    </div>
    <div class='caption'>The L1 cost (black) will likely intersect the <span style='color: red;'>objective cost (red)</span> at an axis, forcing either $w_0$ or $w_1$ to be exactly zero. The entropy surface is much more concave than L1, meaning it will be better at promoting sparsity.*</div>
    <p>The equations for the L1 and Lp norms are:</p>
    \[L_1 = \frac{1}{N}\sum_{i=1}|w_i|\]
    \[L_p = \frac{1}{N}\left(\sum_{i=1}|w_i|^p\right)^{\frac{1}{p}}\] 
    <p>All <math dipslay='inline'><mi>p</mi></math> norms, <math display='inline'><mn>0</mn><mo>&leq;</mo><mi>p</mi><mo>&leq;</mo><mn>1</mn></math>, induce sparsity by intersecting the equal-cost surface of the objective at an axis in high dimensional weight space. This is due to the non-convex ("pointy") nature of the norm. Compare with the L2 norm, which does not encourage sparsity because of its convex surface.</p>
    <p>An alternative to p norm-induced sparsity is entropy regularization. The equation for <a href='https://en.wikipedia.org/wiki/Entropy_(information_theory)'>entropy</a> is:</p>
    \[\mathbb{H} = -\frac{1}{N}\sum_{i=1}p(x_i)\text{log}(p(x_i))\]
    <p>If we replace $p(x)$ by $w_i$ and constrain $0\leq w_i\leq1$, we get the following graph of entropy:</p>
    <div class='figure'>
        <div id='entropy-graph' style='display: inline-block;'></div>
    </div>
    <p>This cost is equal to 0 at the points $w_i=0$ and $w_i=1$, favoring sparsity or full activation. By using a mask or normalization, we can force $w_i$ to be in this range.</p>
    <p>Entropy regularization is used in the <a href='https://github.com/RexYing/gnn-model-explainer'>GNNExplainer algorithm</a> to sparsify the graph edge matrix.</p>
    
    <script type='module' src='/js/posts/EntropyReg.js'></script>
</div>
